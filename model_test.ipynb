{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd47e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c468f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534582f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bea1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 모델: inception_v3 ---\n",
      "  예상 입력 크기 (C, H, W): (3, 299, 299)\n",
      "  정규화 평균 (mean): (0.5, 0.5, 0.5)\n",
      "  정규화 표준 편차 (std): (0.5, 0.5, 0.5)\n",
      "------------------------------\n",
      "--- 모델: mobilenetv2_100 ---\n",
      "  예상 입력 크기 (C, H, W): (3, 224, 224)\n",
      "  정규화 평균 (mean): (0.485, 0.456, 0.406)\n",
      "  정규화 표준 편차 (std): (0.229, 0.224, 0.225)\n",
      "------------------------------\n",
      "--- 모델: efficientnetv2_m ---\n",
      "  예상 입력 크기 (C, H, W): (3, 320, 320)\n",
      "  정규화 평균 (mean): (0.485, 0.456, 0.406)\n",
      "  정규화 표준 편차 (std): (0.229, 0.224, 0.225)\n",
      "------------------------------\n",
      "--- 모델: vgg16 ---\n",
      "  예상 입력 크기 (C, H, W): (3, 224, 224)\n",
      "  정규화 평균 (mean): (0.485, 0.456, 0.406)\n",
      "  정규화 표준 편차 (std): (0.229, 0.224, 0.225)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "\n",
    "model_names = ['inception_v3', 'mobilenetv2_100', 'efficientnetv2_m', 'vgg16']\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        model = timm.create_model(model_name, pretrained=False)\n",
    "        default_cfg = model.default_cfg\n",
    "        \n",
    "        print(f\"--- 모델: {model_name} ---\")\n",
    "        print(f\"  예상 입력 크기 (C, H, W): {default_cfg.get('input_size')}\")\n",
    "        print(f\"  정규화 평균 (mean): {default_cfg.get('mean')}\")\n",
    "        print(f\"  정규화 표준 편차 (std): {default_cfg.get('std')}\")\n",
    "        print(\"-\" * 30)\n",
    "    except Exception as e:\n",
    "        print(f\"--- 모델: {model_name} ---\")\n",
    "        print(f\"  정보를 가져오는 데 실패했습니다: {e}\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eafeb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mydata import create_dataloader_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7747 1937\n",
      "[Epoch 001/075] TrainLoss 0.6080 | ValLoss 0.5461 | Acc 76.05% | BalAcc 75.27% | \n",
      "[Epoch 002/075] TrainLoss 0.5464 | ValLoss 0.5299 | Acc 76.15% | BalAcc 75.20% | \n",
      "[Epoch 003/075] TrainLoss 0.5196 | ValLoss 0.5114 | Acc 77.39% | BalAcc 76.79% | \n",
      "[Epoch 004/075] TrainLoss 0.5052 | ValLoss 0.4532 | Acc 81.78% | BalAcc 81.49% | \n",
      "[Epoch 005/075] TrainLoss 0.4996 | ValLoss 0.4569 | Acc 82.60% | BalAcc 82.59% | \n",
      "[Epoch 006/075] TrainLoss 0.5129 | ValLoss 0.4928 | Acc 78.32% | BalAcc 77.56% | \n",
      "[Epoch 007/075] TrainLoss 0.5075 | ValLoss 0.4573 | Acc 82.19% | BalAcc 82.18% | \n",
      "[Epoch 008/075] TrainLoss 0.4906 | ValLoss 0.4470 | Acc 82.71% | BalAcc 82.59% | \n",
      "[Epoch 009/075] TrainLoss 0.4812 | ValLoss 0.4457 | Acc 82.50% | BalAcc 82.42% | \n",
      "[Epoch 010/075] TrainLoss 0.4772 | ValLoss 0.4331 | Acc 83.38% | BalAcc 83.14% | \n",
      "[Epoch 011/075] TrainLoss 0.4695 | ValLoss 0.4249 | Acc 83.01% | BalAcc 82.90% | \n",
      "[Epoch 012/075] TrainLoss 0.4655 | ValLoss 0.4217 | Acc 83.48% | BalAcc 83.34% | \n",
      "[Epoch 013/075] TrainLoss 0.4568 | ValLoss 0.4187 | Acc 83.63% | BalAcc 83.50% | \n",
      "[Epoch 014/075] TrainLoss 0.4535 | ValLoss 0.4174 | Acc 84.00% | BalAcc 83.84% | \n",
      "[Epoch 015/075] TrainLoss 0.4557 | ValLoss 0.4152 | Acc 83.79% | BalAcc 83.70% | \n",
      "[Epoch 016/075] TrainLoss 0.4710 | ValLoss 0.4238 | Acc 83.94% | BalAcc 83.81% | \n",
      "[Epoch 017/075] TrainLoss 0.4596 | ValLoss 0.4193 | Acc 84.10% | BalAcc 83.91% | \n",
      "[Epoch 018/075] TrainLoss 0.4606 | ValLoss 0.4123 | Acc 84.20% | BalAcc 83.94% | \n",
      "[Epoch 019/075] TrainLoss 0.4505 | ValLoss 0.4051 | Acc 84.10% | BalAcc 83.97% | \n",
      "[Epoch 020/075] TrainLoss 0.4490 | ValLoss 0.3993 | Acc 84.98% | BalAcc 84.84% | \n",
      "[Epoch 021/075] TrainLoss 0.4396 | ValLoss 0.4054 | Acc 84.93% | BalAcc 84.70% | \n",
      "[Epoch 022/075] TrainLoss 0.4349 | ValLoss 0.3889 | Acc 85.65% | BalAcc 85.62% | \n",
      "[Epoch 023/075] TrainLoss 0.4339 | ValLoss 0.3881 | Acc 86.47% | BalAcc 86.28% | \n",
      "[Epoch 024/075] TrainLoss 0.4256 | ValLoss 0.3823 | Acc 86.63% | BalAcc 86.41% | \n",
      "[Epoch 025/075] TrainLoss 0.4174 | ValLoss 0.4047 | Acc 84.72% | BalAcc 85.00% | \n",
      "[Epoch 026/075] TrainLoss 0.4097 | ValLoss 0.3911 | Acc 85.80% | BalAcc 85.39% | \n",
      "[Epoch 027/075] TrainLoss 0.4072 | ValLoss 0.3805 | Acc 85.96% | BalAcc 85.53% | \n",
      "[Epoch 028/075] TrainLoss 0.3944 | ValLoss 0.3652 | Acc 87.51% | BalAcc 87.49% | \n",
      "[Epoch 029/075] TrainLoss 0.3888 | ValLoss 0.3531 | Acc 87.20% | BalAcc 87.15% | \n",
      "[Epoch 030/075] TrainLoss 0.3849 | ValLoss 0.3523 | Acc 87.76% | BalAcc 87.71% | \n",
      "[Epoch 031/075] TrainLoss 0.3793 | ValLoss 0.3464 | Acc 88.38% | BalAcc 88.30% | \n",
      "[Epoch 032/075] TrainLoss 0.3781 | ValLoss 0.3443 | Acc 88.69% | BalAcc 88.59% | \n",
      "[Epoch 033/075] TrainLoss 0.3719 | ValLoss 0.3426 | Acc 89.11% | BalAcc 88.94% | \n",
      "[Epoch 034/075] TrainLoss 0.3647 | ValLoss 0.3488 | Acc 88.33% | BalAcc 88.16% | \n",
      "[Epoch 035/075] TrainLoss 0.3699 | ValLoss 0.3422 | Acc 88.64% | BalAcc 88.52% | \n",
      "[Epoch 036/075] TrainLoss 0.4107 | ValLoss 0.3840 | Acc 86.32% | BalAcc 85.99% | \n",
      "[Epoch 037/075] TrainLoss 0.4172 | ValLoss 0.3688 | Acc 86.47% | BalAcc 86.11% | \n",
      "[Epoch 038/075] TrainLoss 0.4042 | ValLoss 0.4124 | Acc 83.12% | BalAcc 82.40% | \n",
      "[Epoch 039/075] TrainLoss 0.4044 | ValLoss 0.3558 | Acc 87.51% | BalAcc 87.25% | \n",
      "[Epoch 040/075] TrainLoss 0.3938 | ValLoss 0.3724 | Acc 86.63% | BalAcc 86.21% | \n",
      "[Epoch 041/075] TrainLoss 0.3937 | ValLoss 0.3539 | Acc 88.64% | BalAcc 88.35% | \n",
      "[Epoch 042/075] TrainLoss 0.3890 | ValLoss 0.3426 | Acc 88.13% | BalAcc 87.81% | \n",
      "[Epoch 043/075] TrainLoss 0.3873 | ValLoss 0.3437 | Acc 89.11% | BalAcc 88.89% | \n",
      "[Epoch 044/075] TrainLoss 0.3771 | ValLoss 0.3341 | Acc 88.18% | BalAcc 88.12% | \n",
      "[Epoch 045/075] TrainLoss 0.3697 | ValLoss 0.3239 | Acc 89.31% | BalAcc 89.28% | \n",
      "[Epoch 046/075] TrainLoss 0.3717 | ValLoss 0.3272 | Acc 89.83% | BalAcc 89.82% | \n",
      "[Epoch 047/075] TrainLoss 0.3622 | ValLoss 0.3140 | Acc 90.35% | BalAcc 90.12% | \n",
      "best bal acc : 90.12\n",
      "[Epoch 048/075] TrainLoss 0.3566 | ValLoss 0.3135 | Acc 90.55% | BalAcc 90.40% | \n",
      "best bal acc : 90.40\n",
      "[Epoch 049/075] TrainLoss 0.3512 | ValLoss 0.3269 | Acc 89.78% | BalAcc 89.79% | \n",
      "[Epoch 050/075] TrainLoss 0.3362 | ValLoss 0.3068 | Acc 90.40% | BalAcc 90.31% | \n",
      "[Epoch 051/075] TrainLoss 0.3358 | ValLoss 0.3061 | Acc 89.93% | BalAcc 89.89% | \n",
      "[Epoch 052/075] TrainLoss 0.3291 | ValLoss 0.2925 | Acc 90.50% | BalAcc 90.33% | \n",
      "[Epoch 053/075] TrainLoss 0.3219 | ValLoss 0.2925 | Acc 91.22% | BalAcc 91.17% | \n",
      "best bal acc : 91.17\n",
      "[Epoch 054/075] TrainLoss 0.3214 | ValLoss 0.2869 | Acc 91.12% | BalAcc 91.07% | \n",
      "[Epoch 055/075] TrainLoss 0.3116 | ValLoss 0.2931 | Acc 91.58% | BalAcc 91.59% | \n",
      "best bal acc : 91.59\n",
      "[Epoch 056/075] TrainLoss 0.3089 | ValLoss 0.3122 | Acc 89.98% | BalAcc 90.18% | \n",
      "[Epoch 057/075] TrainLoss 0.2965 | ValLoss 0.2837 | Acc 91.84% | BalAcc 91.79% | \n",
      "best bal acc : 91.79\n",
      "[Epoch 058/075] TrainLoss 0.2989 | ValLoss 0.2923 | Acc 91.22% | BalAcc 90.96% | \n",
      "[Epoch 059/075] TrainLoss 0.2902 | ValLoss 0.2755 | Acc 92.20% | BalAcc 92.01% | \n",
      "best bal acc : 92.01\n",
      "[Epoch 060/075] TrainLoss 0.2811 | ValLoss 0.2712 | Acc 92.93% | BalAcc 92.83% | \n",
      "best bal acc : 92.83\n",
      "[Epoch 061/075] TrainLoss 0.2752 | ValLoss 0.2774 | Acc 92.46% | BalAcc 92.52% | \n",
      "[Epoch 062/075] TrainLoss 0.2734 | ValLoss 0.2538 | Acc 93.13% | BalAcc 93.07% | \n",
      "best bal acc : 93.07\n",
      "[Epoch 063/075] TrainLoss 0.2648 | ValLoss 0.2552 | Acc 93.34% | BalAcc 93.37% | \n",
      "best bal acc : 93.37\n",
      "[Epoch 064/075] TrainLoss 0.2554 | ValLoss 0.2641 | Acc 92.57% | BalAcc 92.55% | \n",
      "[Epoch 065/075] TrainLoss 0.2591 | ValLoss 0.2507 | Acc 94.17% | BalAcc 94.16% | \n",
      "best bal acc : 94.16\n",
      "[Epoch 066/075] TrainLoss 0.2503 | ValLoss 0.2526 | Acc 93.50% | BalAcc 93.51% | \n",
      "[Epoch 067/075] TrainLoss 0.2430 | ValLoss 0.2483 | Acc 93.65% | BalAcc 93.65% | \n",
      "[Epoch 068/075] TrainLoss 0.2404 | ValLoss 0.2482 | Acc 93.65% | BalAcc 93.61% | \n",
      "[Epoch 069/075] TrainLoss 0.2318 | ValLoss 0.2527 | Acc 93.34% | BalAcc 93.40% | \n",
      "[Epoch 070/075] TrainLoss 0.2351 | ValLoss 0.2501 | Acc 93.55% | BalAcc 93.57% | \n",
      "[Epoch 071/075] TrainLoss 0.2287 | ValLoss 0.2447 | Acc 93.55% | BalAcc 93.55% | \n",
      "[Epoch 072/075] TrainLoss 0.2323 | ValLoss 0.2449 | Acc 93.80% | BalAcc 93.83% | \n",
      "[Epoch 073/075] TrainLoss 0.2290 | ValLoss 0.2429 | Acc 94.01% | BalAcc 93.98% | \n",
      "[Epoch 074/075] TrainLoss 0.2314 | ValLoss 0.2439 | Acc 93.75% | BalAcc 93.78% | \n",
      "[Epoch 075/075] TrainLoss 0.2256 | ValLoss 0.2478 | Acc 93.70% | BalAcc 93.71% | \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('resnet50', pretrained=True, num_classes=2).to(device)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader, val_loader = create_dataloader_patch(batch_size=batch_size)\n",
    "\n",
    "num_epochs = 75\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5 * len(train_loader), T_mult=2)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,   val_accs   = [], []      # plain accuracy\n",
    "bal_accs, aucs = [], []                # extra metrics\n",
    "\n",
    "best_bal_acc = 90.0                     # (optional) best-model 저장용\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------------- TRAIN ----------------\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # LR sched : **batch level**\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss   += loss.item()\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "        running_total  += y.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_acc      = 100. * running_correct / running_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs  .append(train_acc)\n",
    "\n",
    "    # ---------------- VALIDATION ----------------\n",
    "    model.eval()\n",
    "    val_loss, val_logits, val_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_loss   += loss.item()\n",
    "            val_logits.append(logits.cpu())\n",
    "            val_labels.append(y.cpu())\n",
    "\n",
    "    val_logits = torch.cat(val_logits)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    probs = F.softmax(val_logits, dim=1).numpy()\n",
    "    preds      = val_logits.argmax(1).numpy()\n",
    "    labels     = val_labels.numpy()\n",
    "\n",
    "    # --- metrics ---\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc_plain = accuracy_score(labels, preds) * 100\n",
    "    bal_acc   = balanced_accuracy_score(labels, preds) * 100\n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    # 기록\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs  .append(val_acc_plain)\n",
    "    bal_accs  .append(bal_acc)\n",
    "    # aucs      .append(auc)\n",
    "\n",
    "    # 로그 출력 -------------------------------------------------------\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}/{num_epochs:03d}] \"\n",
    "        f\"TrainLoss {avg_train_loss:.4f} | \"\n",
    "        f\"ValLoss {avg_val_loss:.4f} | \"\n",
    "        f\"Acc {val_acc_plain:.2f}% | \"\n",
    "        f\"BalAcc {bal_acc:.2f}% | \",\n",
    "        # f\"AUC {auc:.3f}\",\n",
    "        flush=True\n",
    "    )\n",
    "    \n",
    "    if bal_acc > best_bal_acc:\n",
    "        best_bal_acc = bal_acc\n",
    "        print(f'best bal acc : {bal_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4309d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7747 1937\n",
      "[Epoch 001/075] TrainLoss 0.5818 | ValLoss 0.5086 | Acc 79.45% | BalAcc 79.76% | \n",
      "[Epoch 002/075] TrainLoss 0.5131 | ValLoss 0.4454 | Acc 83.01% | BalAcc 82.93% | \n",
      "[Epoch 003/075] TrainLoss 0.4906 | ValLoss 0.4328 | Acc 83.43% | BalAcc 83.13% | \n",
      "[Epoch 004/075] TrainLoss 0.4729 | ValLoss 0.4286 | Acc 84.00% | BalAcc 83.98% | \n",
      "[Epoch 005/075] TrainLoss 0.4682 | ValLoss 0.4230 | Acc 84.41% | BalAcc 84.29% | \n",
      "[Epoch 006/075] TrainLoss 0.4799 | ValLoss 0.4381 | Acc 82.81% | BalAcc 82.30% | \n",
      "[Epoch 007/075] TrainLoss 0.4646 | ValLoss 0.4415 | Acc 83.48% | BalAcc 83.75% | \n",
      "[Epoch 008/075] TrainLoss 0.4492 | ValLoss 0.3975 | Acc 86.32% | BalAcc 86.16% | \n",
      "[Epoch 009/075] TrainLoss 0.4416 | ValLoss 0.3966 | Acc 86.32% | BalAcc 86.07% | \n",
      "[Epoch 010/075] TrainLoss 0.4333 | ValLoss 0.3875 | Acc 86.63% | BalAcc 86.66% | \n",
      "[Epoch 011/075] TrainLoss 0.4240 | ValLoss 0.3744 | Acc 87.25% | BalAcc 87.13% | \n",
      "[Epoch 012/075] TrainLoss 0.4152 | ValLoss 0.3729 | Acc 87.56% | BalAcc 87.35% | \n",
      "[Epoch 013/075] TrainLoss 0.4145 | ValLoss 0.3707 | Acc 87.92% | BalAcc 87.81% | \n",
      "[Epoch 014/075] TrainLoss 0.4066 | ValLoss 0.3688 | Acc 87.82% | BalAcc 87.68% | \n",
      "[Epoch 015/075] TrainLoss 0.4085 | ValLoss 0.3685 | Acc 87.87% | BalAcc 87.74% | \n",
      "[Epoch 016/075] TrainLoss 0.4319 | ValLoss 0.4075 | Acc 86.32% | BalAcc 86.60% | \n",
      "[Epoch 017/075] TrainLoss 0.4204 | ValLoss 0.3636 | Acc 88.38% | BalAcc 88.29% | \n",
      "[Epoch 018/075] TrainLoss 0.4026 | ValLoss 0.3623 | Acc 88.18% | BalAcc 88.27% | \n",
      "[Epoch 019/075] TrainLoss 0.4036 | ValLoss 0.3633 | Acc 88.13% | BalAcc 87.79% | \n",
      "[Epoch 020/075] TrainLoss 0.3952 | ValLoss 0.3434 | Acc 89.26% | BalAcc 89.16% | \n",
      "[Epoch 021/075] TrainLoss 0.3891 | ValLoss 0.3386 | Acc 88.80% | BalAcc 88.66% | \n",
      "[Epoch 022/075] TrainLoss 0.3789 | ValLoss 0.3509 | Acc 88.54% | BalAcc 88.51% | \n",
      "[Epoch 023/075] TrainLoss 0.3694 | ValLoss 0.4045 | Acc 85.39% | BalAcc 85.80% | \n",
      "[Epoch 024/075] TrainLoss 0.3598 | ValLoss 0.3351 | Acc 89.47% | BalAcc 89.18% | \n",
      "[Epoch 025/075] TrainLoss 0.3638 | ValLoss 0.3499 | Acc 88.69% | BalAcc 88.85% | \n",
      "[Epoch 026/075] TrainLoss 0.3595 | ValLoss 0.3269 | Acc 89.47% | BalAcc 89.41% | \n",
      "[Epoch 027/075] TrainLoss 0.3462 | ValLoss 0.3201 | Acc 89.57% | BalAcc 89.39% | \n",
      "[Epoch 028/075] TrainLoss 0.3438 | ValLoss 0.3207 | Acc 89.88% | BalAcc 89.87% | \n",
      "[Epoch 029/075] TrainLoss 0.3402 | ValLoss 0.3127 | Acc 90.55% | BalAcc 90.48% | \n",
      "best bal acc : 90.48\n",
      "[Epoch 030/075] TrainLoss 0.3350 | ValLoss 0.3090 | Acc 90.60% | BalAcc 90.52% | \n",
      "best bal acc : 90.52\n",
      "[Epoch 031/075] TrainLoss 0.3379 | ValLoss 0.3104 | Acc 90.50% | BalAcc 90.48% | \n",
      "[Epoch 032/075] TrainLoss 0.3295 | ValLoss 0.3098 | Acc 90.60% | BalAcc 90.56% | \n",
      "best bal acc : 90.56\n",
      "[Epoch 033/075] TrainLoss 0.3310 | ValLoss 0.3090 | Acc 90.60% | BalAcc 90.54% | \n",
      "[Epoch 034/075] TrainLoss 0.3250 | ValLoss 0.3084 | Acc 90.66% | BalAcc 90.59% | \n",
      "best bal acc : 90.59\n",
      "[Epoch 035/075] TrainLoss 0.3306 | ValLoss 0.3088 | Acc 90.60% | BalAcc 90.55% | \n",
      "[Epoch 036/075] TrainLoss 0.3573 | ValLoss 0.3454 | Acc 88.44% | BalAcc 88.69% | \n",
      "[Epoch 037/075] TrainLoss 0.3520 | ValLoss 0.3057 | Acc 91.07% | BalAcc 90.95% | \n",
      "best bal acc : 90.95\n",
      "[Epoch 038/075] TrainLoss 0.3471 | ValLoss 0.3111 | Acc 90.45% | BalAcc 90.46% | \n",
      "[Epoch 039/075] TrainLoss 0.3432 | ValLoss 0.3148 | Acc 89.98% | BalAcc 90.05% | \n",
      "[Epoch 040/075] TrainLoss 0.3390 | ValLoss 0.3098 | Acc 90.50% | BalAcc 90.37% | \n",
      "[Epoch 041/075] TrainLoss 0.3303 | ValLoss 0.3139 | Acc 89.98% | BalAcc 90.06% | \n",
      "[Epoch 042/075] TrainLoss 0.3278 | ValLoss 0.3056 | Acc 90.66% | BalAcc 90.53% | \n",
      "[Epoch 043/075] TrainLoss 0.3250 | ValLoss 0.3413 | Acc 89.62% | BalAcc 89.87% | \n",
      "[Epoch 044/075] TrainLoss 0.3145 | ValLoss 0.3112 | Acc 90.60% | BalAcc 90.75% | \n",
      "[Epoch 045/075] TrainLoss 0.3095 | ValLoss 0.2999 | Acc 90.97% | BalAcc 90.93% | \n",
      "[Epoch 046/075] TrainLoss 0.3034 | ValLoss 0.2942 | Acc 92.10% | BalAcc 92.10% | \n",
      "best bal acc : 92.10\n",
      "[Epoch 047/075] TrainLoss 0.3061 | ValLoss 0.2978 | Acc 91.33% | BalAcc 91.40% | \n",
      "[Epoch 048/075] TrainLoss 0.2959 | ValLoss 0.3117 | Acc 90.86% | BalAcc 90.60% | \n",
      "[Epoch 049/075] TrainLoss 0.2990 | ValLoss 0.2912 | Acc 91.74% | BalAcc 91.65% | \n",
      "[Epoch 050/075] TrainLoss 0.2894 | ValLoss 0.2804 | Acc 92.31% | BalAcc 92.24% | \n",
      "best bal acc : 92.24\n",
      "[Epoch 051/075] TrainLoss 0.2894 | ValLoss 0.2888 | Acc 91.74% | BalAcc 91.79% | \n",
      "[Epoch 052/075] TrainLoss 0.2863 | ValLoss 0.2922 | Acc 91.74% | BalAcc 91.77% | \n",
      "[Epoch 053/075] TrainLoss 0.2780 | ValLoss 0.3051 | Acc 90.66% | BalAcc 90.36% | \n",
      "[Epoch 054/075] TrainLoss 0.2805 | ValLoss 0.2771 | Acc 92.31% | BalAcc 92.19% | \n",
      "[Epoch 055/075] TrainLoss 0.2764 | ValLoss 0.2809 | Acc 92.31% | BalAcc 92.35% | \n",
      "best bal acc : 92.35\n",
      "[Epoch 056/075] TrainLoss 0.2691 | ValLoss 0.2826 | Acc 92.00% | BalAcc 91.86% | \n",
      "[Epoch 057/075] TrainLoss 0.2668 | ValLoss 0.2833 | Acc 92.31% | BalAcc 92.31% | \n",
      "[Epoch 058/075] TrainLoss 0.2609 | ValLoss 0.2786 | Acc 92.20% | BalAcc 92.18% | \n",
      "[Epoch 059/075] TrainLoss 0.2605 | ValLoss 0.2788 | Acc 92.57% | BalAcc 92.54% | \n",
      "best bal acc : 92.54\n",
      "[Epoch 060/075] TrainLoss 0.2567 | ValLoss 0.2810 | Acc 92.67% | BalAcc 92.60% | \n",
      "best bal acc : 92.60\n",
      "[Epoch 061/075] TrainLoss 0.2575 | ValLoss 0.2782 | Acc 92.51% | BalAcc 92.59% | \n",
      "[Epoch 062/075] TrainLoss 0.2594 | ValLoss 0.2884 | Acc 91.84% | BalAcc 91.98% | \n",
      "[Epoch 063/075] TrainLoss 0.2482 | ValLoss 0.2768 | Acc 92.57% | BalAcc 92.58% | \n",
      "[Epoch 064/075] TrainLoss 0.2500 | ValLoss 0.2722 | Acc 93.03% | BalAcc 93.06% | \n",
      "best bal acc : 93.06\n",
      "[Epoch 065/075] TrainLoss 0.2476 | ValLoss 0.2729 | Acc 92.36% | BalAcc 92.28% | \n",
      "[Epoch 066/075] TrainLoss 0.2511 | ValLoss 0.2714 | Acc 92.88% | BalAcc 92.82% | \n",
      "[Epoch 067/075] TrainLoss 0.2452 | ValLoss 0.2725 | Acc 92.98% | BalAcc 92.95% | \n",
      "[Epoch 068/075] TrainLoss 0.2447 | ValLoss 0.2733 | Acc 92.93% | BalAcc 92.87% | \n",
      "[Epoch 069/075] TrainLoss 0.2460 | ValLoss 0.2706 | Acc 93.08% | BalAcc 93.09% | \n",
      "best bal acc : 93.09\n",
      "[Epoch 070/075] TrainLoss 0.2451 | ValLoss 0.2711 | Acc 93.03% | BalAcc 93.07% | \n",
      "[Epoch 071/075] TrainLoss 0.2430 | ValLoss 0.2687 | Acc 93.13% | BalAcc 93.11% | \n",
      "best bal acc : 93.11\n",
      "[Epoch 072/075] TrainLoss 0.2456 | ValLoss 0.2705 | Acc 92.88% | BalAcc 92.89% | \n",
      "[Epoch 073/075] TrainLoss 0.2419 | ValLoss 0.2701 | Acc 92.93% | BalAcc 92.93% | \n",
      "[Epoch 074/075] TrainLoss 0.2466 | ValLoss 0.2701 | Acc 92.93% | BalAcc 92.93% | \n",
      "[Epoch 075/075] TrainLoss 0.2458 | ValLoss 0.2701 | Acc 92.93% | BalAcc 92.93% | \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('vgg16', pretrained=True, num_classes=2).to(device)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader, val_loader = create_dataloader_patch(batch_size=batch_size)\n",
    "\n",
    "num_epochs = 75\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5 * len(train_loader), T_mult=2)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,   val_accs   = [], []      # plain accuracy\n",
    "bal_accs, aucs = [], []                # extra metrics\n",
    "\n",
    "best_bal_acc = 90.0                     # (optional) best-model 저장용\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------------- TRAIN ----------------\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # LR sched : **batch level**\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss   += loss.item()\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "        running_total  += y.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_acc      = 100. * running_correct / running_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs  .append(train_acc)\n",
    "\n",
    "    # ---------------- VALIDATION ----------------\n",
    "    model.eval()\n",
    "    val_loss, val_logits, val_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_loss   += loss.item()\n",
    "            val_logits.append(logits.cpu())\n",
    "            val_labels.append(y.cpu())\n",
    "\n",
    "    val_logits = torch.cat(val_logits)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    probs = F.softmax(val_logits, dim=1).numpy()\n",
    "    preds      = val_logits.argmax(1).numpy()\n",
    "    labels     = val_labels.numpy()\n",
    "\n",
    "    # --- metrics ---\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc_plain = accuracy_score(labels, preds) * 100\n",
    "    bal_acc   = balanced_accuracy_score(labels, preds) * 100\n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    # 기록\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs  .append(val_acc_plain)\n",
    "    bal_accs  .append(bal_acc)\n",
    "    # aucs      .append(auc)\n",
    "\n",
    "    # 로그 출력 -------------------------------------------------------\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}/{num_epochs:03d}] \"\n",
    "        f\"TrainLoss {avg_train_loss:.4f} | \"\n",
    "        f\"ValLoss {avg_val_loss:.4f} | \"\n",
    "        f\"Acc {val_acc_plain:.2f}% | \"\n",
    "        f\"BalAcc {bal_acc:.2f}% | \",\n",
    "        # f\"AUC {auc:.3f}\",\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    if bal_acc > best_bal_acc:\n",
    "        best_bal_acc = bal_acc\n",
    "        print(f'best bal acc : {bal_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7747 1937\n",
      "[Epoch 001/075] TrainLoss 0.8358 | ValLoss 0.5284 | Acc 78.16% | BalAcc 77.59% | \n",
      "[Epoch 002/075] TrainLoss 0.5555 | ValLoss 0.4643 | Acc 82.03% | BalAcc 81.73% | \n",
      "[Epoch 003/075] TrainLoss 0.5206 | ValLoss 0.4494 | Acc 82.03% | BalAcc 81.92% | \n",
      "[Epoch 004/075] TrainLoss 0.4923 | ValLoss 0.4266 | Acc 82.76% | BalAcc 82.45% | \n",
      "[Epoch 005/075] TrainLoss 0.4721 | ValLoss 0.4226 | Acc 82.55% | BalAcc 82.22% | \n",
      "[Epoch 006/075] TrainLoss 0.5441 | ValLoss 0.4424 | Acc 82.45% | BalAcc 82.27% | \n",
      "[Epoch 007/075] TrainLoss 0.5144 | ValLoss 0.4393 | Acc 82.34% | BalAcc 82.40% | \n",
      "[Epoch 008/075] TrainLoss 0.4935 | ValLoss 0.4330 | Acc 82.76% | BalAcc 82.47% | \n",
      "[Epoch 009/075] TrainLoss 0.4795 | ValLoss 0.4039 | Acc 83.22% | BalAcc 82.98% | \n",
      "[Epoch 010/075] TrainLoss 0.4668 | ValLoss 0.4037 | Acc 84.36% | BalAcc 84.27% | \n",
      "[Epoch 011/075] TrainLoss 0.4524 | ValLoss 0.3797 | Acc 85.13% | BalAcc 84.78% | \n",
      "[Epoch 012/075] TrainLoss 0.4279 | ValLoss 0.3601 | Acc 87.61% | BalAcc 87.44% | \n",
      "[Epoch 013/075] TrainLoss 0.4148 | ValLoss 0.3479 | Acc 88.54% | BalAcc 88.35% | \n",
      "[Epoch 014/075] TrainLoss 0.4040 | ValLoss 0.3461 | Acc 88.69% | BalAcc 88.51% | \n",
      "[Epoch 015/075] TrainLoss 0.3901 | ValLoss 0.3436 | Acc 88.49% | BalAcc 88.20% | \n",
      "[Epoch 016/075] TrainLoss 0.4773 | ValLoss 0.3985 | Acc 84.15% | BalAcc 83.67% | \n",
      "[Epoch 017/075] TrainLoss 0.4624 | ValLoss 0.4106 | Acc 83.74% | BalAcc 83.29% | \n",
      "[Epoch 018/075] TrainLoss 0.4556 | ValLoss 0.3884 | Acc 85.23% | BalAcc 84.83% | \n",
      "[Epoch 019/075] TrainLoss 0.4424 | ValLoss 0.3954 | Acc 86.11% | BalAcc 85.53% | \n",
      "[Epoch 020/075] TrainLoss 0.4315 | ValLoss 0.3624 | Acc 87.35% | BalAcc 87.10% | \n",
      "[Epoch 021/075] TrainLoss 0.4248 | ValLoss 0.3846 | Acc 85.85% | BalAcc 85.29% | \n",
      "[Epoch 022/075] TrainLoss 0.4180 | ValLoss 0.3502 | Acc 88.49% | BalAcc 88.32% | \n",
      "[Epoch 023/075] TrainLoss 0.4056 | ValLoss 0.3435 | Acc 87.40% | BalAcc 87.23% | \n",
      "[Epoch 024/075] TrainLoss 0.3966 | ValLoss 0.3558 | Acc 87.97% | BalAcc 87.78% | \n",
      "[Epoch 025/075] TrainLoss 0.3908 | ValLoss 0.3225 | Acc 89.78% | BalAcc 89.57% | \n",
      "[Epoch 026/075] TrainLoss 0.3766 | ValLoss 0.3418 | Acc 88.23% | BalAcc 87.89% | \n",
      "[Epoch 027/075] TrainLoss 0.3614 | ValLoss 0.3265 | Acc 89.26% | BalAcc 88.94% | \n",
      "[Epoch 028/075] TrainLoss 0.3536 | ValLoss 0.3276 | Acc 89.36% | BalAcc 88.98% | \n",
      "[Epoch 029/075] TrainLoss 0.3385 | ValLoss 0.2955 | Acc 91.69% | BalAcc 91.67% | \n",
      "best bal acc : 91.67\n",
      "[Epoch 030/075] TrainLoss 0.3274 | ValLoss 0.2919 | Acc 91.17% | BalAcc 90.93% | \n",
      "[Epoch 031/075] TrainLoss 0.3228 | ValLoss 0.2909 | Acc 90.91% | BalAcc 90.64% | \n",
      "[Epoch 032/075] TrainLoss 0.3082 | ValLoss 0.2779 | Acc 91.74% | BalAcc 91.52% | \n",
      "[Epoch 033/075] TrainLoss 0.3059 | ValLoss 0.2727 | Acc 92.10% | BalAcc 91.96% | \n",
      "best bal acc : 91.96\n",
      "[Epoch 034/075] TrainLoss 0.3015 | ValLoss 0.2757 | Acc 91.64% | BalAcc 91.44% | \n",
      "[Epoch 035/075] TrainLoss 0.2919 | ValLoss 0.2723 | Acc 92.41% | BalAcc 92.28% | \n",
      "best bal acc : 92.28\n",
      "[Epoch 036/075] TrainLoss 0.4110 | ValLoss 0.3450 | Acc 88.90% | BalAcc 88.75% | \n",
      "[Epoch 037/075] TrainLoss 0.4091 | ValLoss 0.3749 | Acc 86.32% | BalAcc 85.85% | \n",
      "[Epoch 038/075] TrainLoss 0.4050 | ValLoss 0.4040 | Acc 85.49% | BalAcc 84.98% | \n",
      "[Epoch 039/075] TrainLoss 0.4011 | ValLoss 0.3591 | Acc 88.59% | BalAcc 88.49% | \n",
      "[Epoch 040/075] TrainLoss 0.3971 | ValLoss 0.3435 | Acc 89.00% | BalAcc 88.89% | \n",
      "[Epoch 041/075] TrainLoss 0.3822 | ValLoss 0.3395 | Acc 88.13% | BalAcc 87.86% | \n",
      "[Epoch 042/075] TrainLoss 0.3813 | ValLoss 0.3294 | Acc 89.47% | BalAcc 89.20% | \n",
      "[Epoch 043/075] TrainLoss 0.3802 | ValLoss 0.3402 | Acc 89.00% | BalAcc 88.63% | \n",
      "[Epoch 044/075] TrainLoss 0.3689 | ValLoss 0.3213 | Acc 89.06% | BalAcc 88.68% | \n",
      "[Epoch 045/075] TrainLoss 0.3700 | ValLoss 0.3272 | Acc 89.83% | BalAcc 89.70% | \n",
      "[Epoch 046/075] TrainLoss 0.3706 | ValLoss 0.3270 | Acc 89.62% | BalAcc 89.40% | \n",
      "[Epoch 047/075] TrainLoss 0.3650 | ValLoss 0.3279 | Acc 88.90% | BalAcc 88.73% | \n",
      "[Epoch 048/075] TrainLoss 0.3550 | ValLoss 0.3375 | Acc 88.64% | BalAcc 88.24% | \n",
      "[Epoch 049/075] TrainLoss 0.3571 | ValLoss 0.3262 | Acc 89.11% | BalAcc 88.78% | \n",
      "[Epoch 050/075] TrainLoss 0.3427 | ValLoss 0.3312 | Acc 88.85% | BalAcc 88.57% | \n",
      "[Epoch 051/075] TrainLoss 0.3423 | ValLoss 0.3098 | Acc 89.88% | BalAcc 89.62% | \n",
      "[Epoch 052/075] TrainLoss 0.3275 | ValLoss 0.3166 | Acc 90.45% | BalAcc 90.14% | \n",
      "[Epoch 053/075] TrainLoss 0.3237 | ValLoss 0.3028 | Acc 91.28% | BalAcc 91.05% | \n",
      "[Epoch 054/075] TrainLoss 0.3271 | ValLoss 0.2884 | Acc 91.69% | BalAcc 91.64% | \n",
      "[Epoch 055/075] TrainLoss 0.3202 | ValLoss 0.2898 | Acc 91.12% | BalAcc 90.94% | \n",
      "[Epoch 056/075] TrainLoss 0.3112 | ValLoss 0.3037 | Acc 91.28% | BalAcc 90.95% | \n",
      "[Epoch 057/075] TrainLoss 0.2994 | ValLoss 0.2938 | Acc 91.48% | BalAcc 91.25% | \n",
      "[Epoch 058/075] TrainLoss 0.2939 | ValLoss 0.2835 | Acc 92.10% | BalAcc 91.92% | \n",
      "[Epoch 059/075] TrainLoss 0.2900 | ValLoss 0.2690 | Acc 92.88% | BalAcc 92.66% | \n",
      "best bal acc : 92.66\n",
      "[Epoch 060/075] TrainLoss 0.2823 | ValLoss 0.2929 | Acc 91.33% | BalAcc 91.06% | \n",
      "[Epoch 061/075] TrainLoss 0.2901 | ValLoss 0.2636 | Acc 93.08% | BalAcc 92.96% | \n",
      "best bal acc : 92.96\n",
      "[Epoch 062/075] TrainLoss 0.2752 | ValLoss 0.2571 | Acc 93.29% | BalAcc 93.19% | \n",
      "best bal acc : 93.19\n",
      "[Epoch 063/075] TrainLoss 0.2614 | ValLoss 0.2663 | Acc 93.13% | BalAcc 92.98% | \n",
      "[Epoch 064/075] TrainLoss 0.2609 | ValLoss 0.2579 | Acc 93.60% | BalAcc 93.53% | \n",
      "best bal acc : 93.53\n",
      "[Epoch 065/075] TrainLoss 0.2533 | ValLoss 0.2691 | Acc 92.98% | BalAcc 92.74% | \n",
      "[Epoch 066/075] TrainLoss 0.2496 | ValLoss 0.2559 | Acc 93.44% | BalAcc 93.31% | \n",
      "[Epoch 067/075] TrainLoss 0.2459 | ValLoss 0.2469 | Acc 93.60% | BalAcc 93.51% | \n",
      "[Epoch 068/075] TrainLoss 0.2354 | ValLoss 0.2454 | Acc 93.96% | BalAcc 93.84% | \n",
      "best bal acc : 93.84\n",
      "[Epoch 069/075] TrainLoss 0.2353 | ValLoss 0.2390 | Acc 94.63% | BalAcc 94.51% | \n",
      "best bal acc : 94.51\n",
      "[Epoch 070/075] TrainLoss 0.2242 | ValLoss 0.2374 | Acc 94.53% | BalAcc 94.41% | \n",
      "[Epoch 071/075] TrainLoss 0.2270 | ValLoss 0.2362 | Acc 94.63% | BalAcc 94.54% | \n",
      "best bal acc : 94.54\n",
      "[Epoch 072/075] TrainLoss 0.2293 | ValLoss 0.2362 | Acc 94.53% | BalAcc 94.43% | \n",
      "[Epoch 073/075] TrainLoss 0.2229 | ValLoss 0.2350 | Acc 94.79% | BalAcc 94.68% | \n",
      "best bal acc : 94.68\n",
      "[Epoch 074/075] TrainLoss 0.2205 | ValLoss 0.2357 | Acc 94.63% | BalAcc 94.54% | \n",
      "[Epoch 075/075] TrainLoss 0.2198 | ValLoss 0.2358 | Acc 94.68% | BalAcc 94.58% | \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('efficientnetv2_m', pretrained=False, num_classes=2).to(device)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader, val_loader = create_dataloader_patch(batch_size=batch_size, out_size=320)\n",
    "\n",
    "num_epochs = 75\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.SGD(model.parameters(), lr=4e-3, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5 * len(train_loader), T_mult=2)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,   val_accs   = [], []      # plain accuracy\n",
    "bal_accs, aucs = [], []                # extra metrics\n",
    "\n",
    "best_bal_acc = 90.0                     # (optional) best-model 저장용\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------------- TRAIN ----------------\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # LR sched : **batch level**\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss   += loss.item()\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "        running_total  += y.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_acc      = 100. * running_correct / running_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs  .append(train_acc)\n",
    "\n",
    "    # ---------------- VALIDATION ----------------\n",
    "    model.eval()\n",
    "    val_loss, val_logits, val_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_loss   += loss.item()\n",
    "            val_logits.append(logits.cpu())\n",
    "            val_labels.append(y.cpu())\n",
    "\n",
    "    val_logits = torch.cat(val_logits)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    probs = F.softmax(val_logits, dim=1).numpy()\n",
    "    preds      = val_logits.argmax(1).numpy()\n",
    "    labels     = val_labels.numpy()\n",
    "\n",
    "    # --- metrics ---\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc_plain = accuracy_score(labels, preds) * 100\n",
    "    bal_acc   = balanced_accuracy_score(labels, preds) * 100\n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    # 기록\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs  .append(val_acc_plain)\n",
    "    bal_accs  .append(bal_acc)\n",
    "    # aucs      .append(auc)\n",
    "\n",
    "    # 로그 출력 -------------------------------------------------------\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}/{num_epochs:03d}] \"\n",
    "        f\"TrainLoss {avg_train_loss:.4f} | \"\n",
    "        f\"ValLoss {avg_val_loss:.4f} | \"\n",
    "        f\"Acc {val_acc_plain:.2f}% | \"\n",
    "        f\"BalAcc {bal_acc:.2f}% | \",\n",
    "        # f\"AUC {auc:.3f}\",\n",
    "        flush=True\n",
    "    )\n",
    "    \n",
    "    if bal_acc > best_bal_acc:\n",
    "        best_bal_acc = bal_acc\n",
    "        print(f'best bal acc : {bal_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b960db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863f1777520243f8ba67224e9320d104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7747 1937\n",
      "[Epoch 001/075] TrainLoss 0.7100 | ValLoss 0.4731 | Acc 81.21% | BalAcc 81.13% | \n",
      "[Epoch 002/075] TrainLoss 0.5343 | ValLoss 0.4783 | Acc 81.88% | BalAcc 81.89% | \n",
      "[Epoch 003/075] TrainLoss 0.5130 | ValLoss 0.4536 | Acc 82.03% | BalAcc 81.91% | \n",
      "[Epoch 004/075] TrainLoss 0.4954 | ValLoss 0.4504 | Acc 82.40% | BalAcc 82.23% | \n",
      "[Epoch 005/075] TrainLoss 0.4837 | ValLoss 0.4515 | Acc 82.55% | BalAcc 82.45% | \n",
      "[Epoch 006/075] TrainLoss 0.5080 | ValLoss 0.4571 | Acc 82.29% | BalAcc 82.29% | \n",
      "[Epoch 007/075] TrainLoss 0.4946 | ValLoss 0.4378 | Acc 82.86% | BalAcc 82.59% | \n",
      "[Epoch 008/075] TrainLoss 0.4884 | ValLoss 0.4421 | Acc 82.91% | BalAcc 82.96% | \n",
      "[Epoch 009/075] TrainLoss 0.4775 | ValLoss 0.4354 | Acc 83.58% | BalAcc 83.46% | \n",
      "[Epoch 010/075] TrainLoss 0.4724 | ValLoss 0.4240 | Acc 84.31% | BalAcc 84.06% | \n",
      "[Epoch 011/075] TrainLoss 0.4652 | ValLoss 0.4227 | Acc 83.43% | BalAcc 83.10% | \n",
      "[Epoch 012/075] TrainLoss 0.4627 | ValLoss 0.4144 | Acc 84.20% | BalAcc 84.04% | \n",
      "[Epoch 013/075] TrainLoss 0.4543 | ValLoss 0.4112 | Acc 84.72% | BalAcc 84.70% | \n",
      "[Epoch 014/075] TrainLoss 0.4503 | ValLoss 0.4112 | Acc 84.67% | BalAcc 84.55% | \n",
      "[Epoch 015/075] TrainLoss 0.4514 | ValLoss 0.4087 | Acc 84.46% | BalAcc 84.31% | \n",
      "[Epoch 016/075] TrainLoss 0.4693 | ValLoss 0.4049 | Acc 85.13% | BalAcc 85.02% | \n",
      "[Epoch 017/075] TrainLoss 0.4680 | ValLoss 0.4170 | Acc 83.84% | BalAcc 83.44% | \n",
      "[Epoch 018/075] TrainLoss 0.4571 | ValLoss 0.4007 | Acc 85.13% | BalAcc 84.94% | \n",
      "[Epoch 019/075] TrainLoss 0.4549 | ValLoss 0.4300 | Acc 84.20% | BalAcc 84.37% | \n",
      "[Epoch 020/075] TrainLoss 0.4463 | ValLoss 0.4007 | Acc 85.23% | BalAcc 85.00% | \n",
      "[Epoch 021/075] TrainLoss 0.4450 | ValLoss 0.3992 | Acc 84.77% | BalAcc 84.36% | \n",
      "[Epoch 022/075] TrainLoss 0.4380 | ValLoss 0.3868 | Acc 85.60% | BalAcc 85.25% | \n",
      "[Epoch 023/075] TrainLoss 0.4325 | ValLoss 0.3925 | Acc 85.80% | BalAcc 85.76% | \n",
      "[Epoch 024/075] TrainLoss 0.4327 | ValLoss 0.3811 | Acc 86.22% | BalAcc 86.07% | \n",
      "[Epoch 025/075] TrainLoss 0.4279 | ValLoss 0.3865 | Acc 86.32% | BalAcc 86.16% | \n",
      "[Epoch 026/075] TrainLoss 0.4224 | ValLoss 0.3803 | Acc 86.16% | BalAcc 85.95% | \n",
      "[Epoch 027/075] TrainLoss 0.4170 | ValLoss 0.3715 | Acc 86.89% | BalAcc 86.81% | \n",
      "[Epoch 028/075] TrainLoss 0.4134 | ValLoss 0.3760 | Acc 87.40% | BalAcc 87.42% | \n",
      "[Epoch 029/075] TrainLoss 0.4078 | ValLoss 0.3720 | Acc 86.99% | BalAcc 86.89% | \n",
      "[Epoch 030/075] TrainLoss 0.4125 | ValLoss 0.3691 | Acc 87.30% | BalAcc 87.16% | \n",
      "[Epoch 031/075] TrainLoss 0.4043 | ValLoss 0.3696 | Acc 87.61% | BalAcc 87.56% | \n",
      "[Epoch 032/075] TrainLoss 0.4056 | ValLoss 0.3660 | Acc 86.99% | BalAcc 86.81% | \n",
      "[Epoch 033/075] TrainLoss 0.4003 | ValLoss 0.3641 | Acc 87.30% | BalAcc 87.15% | \n",
      "[Epoch 034/075] TrainLoss 0.4009 | ValLoss 0.3640 | Acc 87.25% | BalAcc 87.13% | \n",
      "[Epoch 035/075] TrainLoss 0.4001 | ValLoss 0.3652 | Acc 87.15% | BalAcc 87.02% | \n",
      "[Epoch 036/075] TrainLoss 0.4258 | ValLoss 0.3808 | Acc 85.96% | BalAcc 85.69% | \n",
      "[Epoch 037/075] TrainLoss 0.4283 | ValLoss 0.3860 | Acc 86.58% | BalAcc 86.28% | \n",
      "[Epoch 038/075] TrainLoss 0.4210 | ValLoss 0.3904 | Acc 85.70% | BalAcc 85.29% | \n",
      "[Epoch 039/075] TrainLoss 0.4141 | ValLoss 0.3786 | Acc 86.73% | BalAcc 86.87% | \n",
      "[Epoch 040/075] TrainLoss 0.4097 | ValLoss 0.3582 | Acc 87.30% | BalAcc 87.05% | \n",
      "[Epoch 041/075] TrainLoss 0.4144 | ValLoss 0.3769 | Acc 85.85% | BalAcc 85.43% | \n",
      "[Epoch 042/075] TrainLoss 0.4101 | ValLoss 0.3614 | Acc 87.71% | BalAcc 87.68% | \n",
      "[Epoch 043/075] TrainLoss 0.4029 | ValLoss 0.3583 | Acc 87.87% | BalAcc 87.85% | \n",
      "[Epoch 044/075] TrainLoss 0.3977 | ValLoss 0.3574 | Acc 87.56% | BalAcc 87.59% | \n",
      "[Epoch 045/075] TrainLoss 0.3889 | ValLoss 0.3527 | Acc 87.66% | BalAcc 87.50% | \n",
      "[Epoch 046/075] TrainLoss 0.3895 | ValLoss 0.3542 | Acc 86.89% | BalAcc 86.60% | \n",
      "[Epoch 047/075] TrainLoss 0.3878 | ValLoss 0.3398 | Acc 88.13% | BalAcc 87.98% | \n",
      "[Epoch 048/075] TrainLoss 0.3820 | ValLoss 0.3436 | Acc 88.28% | BalAcc 88.14% | \n",
      "[Epoch 049/075] TrainLoss 0.3769 | ValLoss 0.3496 | Acc 87.71% | BalAcc 87.46% | \n",
      "[Epoch 050/075] TrainLoss 0.3791 | ValLoss 0.3480 | Acc 88.44% | BalAcc 88.42% | \n",
      "[Epoch 051/075] TrainLoss 0.3704 | ValLoss 0.3334 | Acc 88.80% | BalAcc 88.61% | \n",
      "[Epoch 052/075] TrainLoss 0.3685 | ValLoss 0.3322 | Acc 88.90% | BalAcc 88.75% | \n",
      "[Epoch 053/075] TrainLoss 0.3622 | ValLoss 0.3279 | Acc 89.52% | BalAcc 89.44% | \n",
      "[Epoch 054/075] TrainLoss 0.3612 | ValLoss 0.3268 | Acc 89.21% | BalAcc 89.02% | \n",
      "[Epoch 055/075] TrainLoss 0.3619 | ValLoss 0.3224 | Acc 90.04% | BalAcc 90.05% | \n",
      "best bal acc : 90.05\n",
      "[Epoch 056/075] TrainLoss 0.3532 | ValLoss 0.3204 | Acc 90.35% | BalAcc 90.29% | \n",
      "best bal acc : 90.29\n",
      "[Epoch 057/075] TrainLoss 0.3535 | ValLoss 0.3194 | Acc 90.09% | BalAcc 89.89% | \n",
      "[Epoch 058/075] TrainLoss 0.3502 | ValLoss 0.3204 | Acc 89.83% | BalAcc 89.69% | \n",
      "[Epoch 059/075] TrainLoss 0.3457 | ValLoss 0.3249 | Acc 89.21% | BalAcc 89.19% | \n",
      "[Epoch 060/075] TrainLoss 0.3433 | ValLoss 0.3159 | Acc 89.78% | BalAcc 89.71% | \n",
      "[Epoch 061/075] TrainLoss 0.3397 | ValLoss 0.3121 | Acc 89.88% | BalAcc 89.84% | \n",
      "[Epoch 062/075] TrainLoss 0.3327 | ValLoss 0.3125 | Acc 90.24% | BalAcc 90.19% | \n",
      "[Epoch 063/075] TrainLoss 0.3337 | ValLoss 0.3104 | Acc 90.40% | BalAcc 90.43% | \n",
      "best bal acc : 90.43\n",
      "[Epoch 064/075] TrainLoss 0.3325 | ValLoss 0.3043 | Acc 90.76% | BalAcc 90.63% | \n",
      "best bal acc : 90.63\n",
      "[Epoch 065/075] TrainLoss 0.3306 | ValLoss 0.3100 | Acc 90.81% | BalAcc 90.84% | \n",
      "best bal acc : 90.84\n",
      "[Epoch 066/075] TrainLoss 0.3281 | ValLoss 0.3054 | Acc 90.66% | BalAcc 90.63% | \n",
      "[Epoch 067/075] TrainLoss 0.3268 | ValLoss 0.3054 | Acc 90.81% | BalAcc 90.83% | \n",
      "[Epoch 068/075] TrainLoss 0.3226 | ValLoss 0.3027 | Acc 91.02% | BalAcc 90.95% | \n",
      "best bal acc : 90.95\n",
      "[Epoch 069/075] TrainLoss 0.3192 | ValLoss 0.3063 | Acc 91.02% | BalAcc 91.07% | \n",
      "best bal acc : 91.07\n",
      "[Epoch 070/075] TrainLoss 0.3195 | ValLoss 0.3019 | Acc 90.81% | BalAcc 90.80% | \n",
      "[Epoch 071/075] TrainLoss 0.3151 | ValLoss 0.3006 | Acc 91.33% | BalAcc 91.31% | \n",
      "best bal acc : 91.31\n",
      "[Epoch 072/075] TrainLoss 0.3137 | ValLoss 0.3026 | Acc 91.28% | BalAcc 91.23% | \n",
      "[Epoch 073/075] TrainLoss 0.3213 | ValLoss 0.3008 | Acc 91.12% | BalAcc 91.10% | \n",
      "[Epoch 074/075] TrainLoss 0.3156 | ValLoss 0.3010 | Acc 91.28% | BalAcc 91.30% | \n",
      "[Epoch 075/075] TrainLoss 0.3142 | ValLoss 0.3009 | Acc 91.12% | BalAcc 91.14% | \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=2).to(device)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader, val_loader = create_dataloader_patch(batch_size=batch_size)\n",
    "\n",
    "num_epochs = 75\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5 * len(train_loader), T_mult=2)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,   val_accs   = [], []      # plain accuracy\n",
    "bal_accs, aucs = [], []                # extra metrics\n",
    "\n",
    "best_bal_acc = 90.0                     # (optional) best-model 저장용\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------------- TRAIN ----------------\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # LR sched : **batch level**\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss   += loss.item()\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "        running_total  += y.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_acc      = 100. * running_correct / running_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs  .append(train_acc)\n",
    "\n",
    "    # ---------------- VALIDATION ----------------\n",
    "    model.eval()\n",
    "    val_loss, val_logits, val_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_loss   += loss.item()\n",
    "            val_logits.append(logits.cpu())\n",
    "            val_labels.append(y.cpu())\n",
    "\n",
    "    val_logits = torch.cat(val_logits)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    probs = F.softmax(val_logits, dim=1).numpy()\n",
    "    preds      = val_logits.argmax(1).numpy()\n",
    "    labels     = val_labels.numpy()\n",
    "\n",
    "    # --- metrics ---\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc_plain = accuracy_score(labels, preds) * 100\n",
    "    bal_acc   = balanced_accuracy_score(labels, preds) * 100\n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    # 기록\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs  .append(val_acc_plain)\n",
    "    bal_accs  .append(bal_acc)\n",
    "    # aucs      .append(auc)\n",
    "\n",
    "    # 로그 출력 -------------------------------------------------------\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}/{num_epochs:03d}] \"\n",
    "        f\"TrainLoss {avg_train_loss:.4f} | \"\n",
    "        f\"ValLoss {avg_val_loss:.4f} | \"\n",
    "        f\"Acc {val_acc_plain:.2f}% | \"\n",
    "        f\"BalAcc {bal_acc:.2f}% | \",\n",
    "        # f\"AUC {auc:.3f}\",\n",
    "        flush=True\n",
    "    )\n",
    "    \n",
    "    if bal_acc > best_bal_acc:\n",
    "        best_bal_acc = bal_acc\n",
    "        print(f'best bal acc : {bal_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756007d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7747 1937\n",
      "[Epoch 001/075] TrainLoss 0.6104 | ValLoss 0.4998 | Acc 79.97% | BalAcc 80.33% | \n",
      "[Epoch 002/075] TrainLoss 0.5106 | ValLoss 0.4666 | Acc 80.80% | BalAcc 80.06% | \n",
      "[Epoch 003/075] TrainLoss 0.4779 | ValLoss 0.4119 | Acc 84.87% | BalAcc 84.78% | \n",
      "[Epoch 004/075] TrainLoss 0.4500 | ValLoss 0.3962 | Acc 85.60% | BalAcc 85.53% | \n",
      "[Epoch 005/075] TrainLoss 0.4363 | ValLoss 0.3914 | Acc 86.58% | BalAcc 86.49% | \n",
      "[Epoch 006/075] TrainLoss 0.4762 | ValLoss 0.4200 | Acc 84.15% | BalAcc 83.75% | \n",
      "[Epoch 007/075] TrainLoss 0.4555 | ValLoss 0.4005 | Acc 85.18% | BalAcc 85.08% | \n",
      "[Epoch 008/075] TrainLoss 0.4474 | ValLoss 0.3936 | Acc 85.54% | BalAcc 85.32% | \n",
      "[Epoch 009/075] TrainLoss 0.4263 | ValLoss 0.4151 | Acc 83.84% | BalAcc 83.22% | \n",
      "[Epoch 010/075] TrainLoss 0.4161 | ValLoss 0.3593 | Acc 87.56% | BalAcc 87.29% | \n",
      "[Epoch 011/075] TrainLoss 0.4034 | ValLoss 0.3429 | Acc 88.59% | BalAcc 88.39% | \n",
      "[Epoch 012/075] TrainLoss 0.3894 | ValLoss 0.3348 | Acc 89.26% | BalAcc 89.05% | \n",
      "[Epoch 013/075] TrainLoss 0.3690 | ValLoss 0.3340 | Acc 89.36% | BalAcc 89.33% | \n",
      "[Epoch 014/075] TrainLoss 0.3654 | ValLoss 0.3237 | Acc 89.83% | BalAcc 89.76% | \n",
      "[Epoch 015/075] TrainLoss 0.3609 | ValLoss 0.3236 | Acc 89.93% | BalAcc 89.84% | \n",
      "[Epoch 016/075] TrainLoss 0.4175 | ValLoss 0.3834 | Acc 86.47% | BalAcc 86.08% | \n",
      "[Epoch 017/075] TrainLoss 0.4132 | ValLoss 0.3569 | Acc 88.59% | BalAcc 88.36% | \n",
      "[Epoch 018/075] TrainLoss 0.4016 | ValLoss 0.3575 | Acc 88.18% | BalAcc 87.95% | \n",
      "[Epoch 019/075] TrainLoss 0.3887 | ValLoss 0.3982 | Acc 85.29% | BalAcc 85.70% | \n",
      "[Epoch 020/075] TrainLoss 0.3824 | ValLoss 0.3466 | Acc 88.49% | BalAcc 88.63% | \n",
      "[Epoch 021/075] TrainLoss 0.3711 | ValLoss 0.3336 | Acc 89.21% | BalAcc 89.00% | \n",
      "[Epoch 022/075] TrainLoss 0.3638 | ValLoss 0.3184 | Acc 89.62% | BalAcc 89.42% | \n",
      "[Epoch 023/075] TrainLoss 0.3519 | ValLoss 0.3162 | Acc 89.73% | BalAcc 89.43% | \n",
      "[Epoch 024/075] TrainLoss 0.3435 | ValLoss 0.3054 | Acc 90.66% | BalAcc 90.44% | \n",
      "best bal acc : 90.44\n",
      "[Epoch 025/075] TrainLoss 0.3375 | ValLoss 0.3003 | Acc 91.22% | BalAcc 91.27% | \n",
      "best bal acc : 91.27\n",
      "[Epoch 026/075] TrainLoss 0.3256 | ValLoss 0.2986 | Acc 91.28% | BalAcc 91.31% | \n",
      "best bal acc : 91.31\n",
      "[Epoch 027/075] TrainLoss 0.3125 | ValLoss 0.2911 | Acc 91.64% | BalAcc 91.55% | \n",
      "best bal acc : 91.55\n",
      "[Epoch 028/075] TrainLoss 0.3055 | ValLoss 0.2908 | Acc 91.64% | BalAcc 91.41% | \n",
      "[Epoch 029/075] TrainLoss 0.2992 | ValLoss 0.2716 | Acc 92.20% | BalAcc 92.19% | \n",
      "best bal acc : 92.19\n",
      "[Epoch 030/075] TrainLoss 0.2875 | ValLoss 0.2735 | Acc 92.46% | BalAcc 92.30% | \n",
      "best bal acc : 92.30\n",
      "[Epoch 031/075] TrainLoss 0.2788 | ValLoss 0.2647 | Acc 92.41% | BalAcc 92.34% | \n",
      "best bal acc : 92.34\n",
      "[Epoch 032/075] TrainLoss 0.2746 | ValLoss 0.2670 | Acc 92.72% | BalAcc 92.71% | \n",
      "best bal acc : 92.71\n",
      "[Epoch 033/075] TrainLoss 0.2729 | ValLoss 0.2671 | Acc 92.57% | BalAcc 92.57% | \n",
      "[Epoch 034/075] TrainLoss 0.2698 | ValLoss 0.2660 | Acc 92.72% | BalAcc 92.72% | \n",
      "best bal acc : 92.72\n",
      "[Epoch 035/075] TrainLoss 0.2635 | ValLoss 0.2650 | Acc 92.77% | BalAcc 92.76% | \n",
      "best bal acc : 92.76\n",
      "[Epoch 036/075] TrainLoss 0.3492 | ValLoss 0.3664 | Acc 86.84% | BalAcc 87.21% | \n",
      "[Epoch 037/075] TrainLoss 0.3489 | ValLoss 0.2922 | Acc 90.97% | BalAcc 91.00% | \n",
      "[Epoch 038/075] TrainLoss 0.3392 | ValLoss 0.3137 | Acc 90.50% | BalAcc 90.65% | \n",
      "[Epoch 039/075] TrainLoss 0.3315 | ValLoss 0.3082 | Acc 90.04% | BalAcc 90.00% | \n",
      "[Epoch 040/075] TrainLoss 0.3294 | ValLoss 0.3412 | Acc 88.49% | BalAcc 88.12% | \n",
      "[Epoch 041/075] TrainLoss 0.3243 | ValLoss 0.3026 | Acc 91.12% | BalAcc 90.99% | \n",
      "[Epoch 042/075] TrainLoss 0.3118 | ValLoss 0.3105 | Acc 90.60% | BalAcc 90.67% | \n",
      "[Epoch 043/075] TrainLoss 0.3091 | ValLoss 0.2906 | Acc 91.02% | BalAcc 90.85% | \n",
      "[Epoch 044/075] TrainLoss 0.3038 | ValLoss 0.2967 | Acc 91.07% | BalAcc 91.20% | \n",
      "[Epoch 045/075] TrainLoss 0.2993 | ValLoss 0.3071 | Acc 90.66% | BalAcc 90.37% | \n",
      "[Epoch 046/075] TrainLoss 0.2946 | ValLoss 0.2823 | Acc 92.51% | BalAcc 92.31% | \n",
      "[Epoch 047/075] TrainLoss 0.2884 | ValLoss 0.2818 | Acc 91.22% | BalAcc 91.21% | \n",
      "[Epoch 048/075] TrainLoss 0.2793 | ValLoss 0.2666 | Acc 93.03% | BalAcc 92.92% | \n",
      "best bal acc : 92.92\n",
      "[Epoch 049/075] TrainLoss 0.2829 | ValLoss 0.2754 | Acc 91.95% | BalAcc 91.77% | \n",
      "[Epoch 050/075] TrainLoss 0.2667 | ValLoss 0.2764 | Acc 92.62% | BalAcc 92.51% | \n",
      "[Epoch 051/075] TrainLoss 0.2629 | ValLoss 0.2864 | Acc 91.95% | BalAcc 91.87% | \n",
      "[Epoch 052/075] TrainLoss 0.2628 | ValLoss 0.2801 | Acc 92.46% | BalAcc 92.28% | \n",
      "[Epoch 053/075] TrainLoss 0.2592 | ValLoss 0.2600 | Acc 93.44% | BalAcc 93.38% | \n",
      "best bal acc : 93.38\n",
      "[Epoch 054/075] TrainLoss 0.2500 | ValLoss 0.2584 | Acc 92.77% | BalAcc 92.71% | \n",
      "[Epoch 055/075] TrainLoss 0.2388 | ValLoss 0.2647 | Acc 93.08% | BalAcc 93.09% | \n",
      "[Epoch 056/075] TrainLoss 0.2344 | ValLoss 0.2639 | Acc 92.88% | BalAcc 92.79% | \n",
      "[Epoch 057/075] TrainLoss 0.2295 | ValLoss 0.2636 | Acc 92.93% | BalAcc 92.87% | \n",
      "[Epoch 058/075] TrainLoss 0.2304 | ValLoss 0.2577 | Acc 93.70% | BalAcc 93.65% | \n",
      "best bal acc : 93.65\n",
      "[Epoch 059/075] TrainLoss 0.2200 | ValLoss 0.2523 | Acc 94.17% | BalAcc 94.05% | \n",
      "best bal acc : 94.05\n",
      "[Epoch 060/075] TrainLoss 0.2110 | ValLoss 0.2538 | Acc 93.65% | BalAcc 93.58% | \n",
      "[Epoch 061/075] TrainLoss 0.2162 | ValLoss 0.2380 | Acc 94.42% | BalAcc 94.35% | \n",
      "best bal acc : 94.35\n",
      "[Epoch 062/075] TrainLoss 0.2105 | ValLoss 0.2499 | Acc 94.42% | BalAcc 94.29% | \n",
      "[Epoch 063/075] TrainLoss 0.2117 | ValLoss 0.2431 | Acc 94.37% | BalAcc 94.27% | \n",
      "[Epoch 064/075] TrainLoss 0.2084 | ValLoss 0.2404 | Acc 94.42% | BalAcc 94.43% | \n",
      "best bal acc : 94.43\n",
      "[Epoch 065/075] TrainLoss 0.1962 | ValLoss 0.2396 | Acc 94.58% | BalAcc 94.55% | \n",
      "best bal acc : 94.55\n",
      "[Epoch 066/075] TrainLoss 0.1970 | ValLoss 0.2385 | Acc 94.58% | BalAcc 94.56% | \n",
      "best bal acc : 94.56\n",
      "[Epoch 067/075] TrainLoss 0.1879 | ValLoss 0.2355 | Acc 94.84% | BalAcc 94.81% | \n",
      "best bal acc : 94.81\n",
      "[Epoch 068/075] TrainLoss 0.1939 | ValLoss 0.2344 | Acc 95.10% | BalAcc 95.06% | \n",
      "best bal acc : 95.06\n",
      "[Epoch 069/075] TrainLoss 0.1949 | ValLoss 0.2351 | Acc 94.94% | BalAcc 94.89% | \n",
      "[Epoch 070/075] TrainLoss 0.1921 | ValLoss 0.2377 | Acc 94.89% | BalAcc 94.86% | \n",
      "[Epoch 071/075] TrainLoss 0.1842 | ValLoss 0.2373 | Acc 94.73% | BalAcc 94.74% | \n",
      "[Epoch 072/075] TrainLoss 0.1842 | ValLoss 0.2356 | Acc 94.89% | BalAcc 94.87% | \n",
      "[Epoch 073/075] TrainLoss 0.1780 | ValLoss 0.2352 | Acc 95.25% | BalAcc 95.23% | \n",
      "best bal acc : 95.23\n",
      "[Epoch 074/075] TrainLoss 0.1833 | ValLoss 0.2359 | Acc 95.20% | BalAcc 95.17% | \n",
      "[Epoch 075/075] TrainLoss 0.1778 | ValLoss 0.2380 | Acc 94.84% | BalAcc 94.83% | \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('inception_v3', pretrained=True, num_classes=2).to(device)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader, val_loader = create_dataloader_patch(batch_size=batch_size, out_size=299)\n",
    "\n",
    "num_epochs = 75\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5 * len(train_loader), T_mult=2)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,   val_accs   = [], []      # plain accuracy\n",
    "bal_accs, aucs = [], []                # extra metrics\n",
    "\n",
    "best_bal_acc = 90.0                     # (optional) best-model 저장용\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------------- TRAIN ----------------\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # LR sched : **batch level**\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss   += loss.item()\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "        running_total  += y.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_acc      = 100. * running_correct / running_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs  .append(train_acc)\n",
    "\n",
    "    # ---------------- VALIDATION ----------------\n",
    "    model.eval()\n",
    "    val_loss, val_logits, val_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_loss   += loss.item()\n",
    "            val_logits.append(logits.cpu())\n",
    "            val_labels.append(y.cpu())\n",
    "\n",
    "    val_logits = torch.cat(val_logits)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    probs = F.softmax(val_logits, dim=1).numpy()\n",
    "    preds      = val_logits.argmax(1).numpy()\n",
    "    labels     = val_labels.numpy()\n",
    "\n",
    "    # --- metrics ---\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc_plain = accuracy_score(labels, preds) * 100\n",
    "    bal_acc   = balanced_accuracy_score(labels, preds) * 100\n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    # 기록\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs  .append(val_acc_plain)\n",
    "    bal_accs  .append(bal_acc)\n",
    "    # aucs      .append(auc)\n",
    "\n",
    "    # 로그 출력 -------------------------------------------------------\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}/{num_epochs:03d}] \"\n",
    "        f\"TrainLoss {avg_train_loss:.4f} | \"\n",
    "        f\"ValLoss {avg_val_loss:.4f} | \"\n",
    "        f\"Acc {val_acc_plain:.2f}% | \"\n",
    "        f\"BalAcc {bal_acc:.2f}% | \",\n",
    "        # f\"AUC {auc:.3f}\",\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    if bal_acc > best_bal_acc:\n",
    "        best_bal_acc = bal_acc\n",
    "        print(f'best bal acc : {bal_acc:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
